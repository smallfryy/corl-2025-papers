# corl-2025-papers
Collection of CoRL 2025 papers, summaries, code, etc

## Orals

<table>
  <tr>
    <th>#</th>
    <th>Paper Title</th>
    <th>TLDR</th>
    <th>Project Page</th>
    <th>ArXiv</th>
    <th>Code</th>
  </tr>
  <tr>
    <td>1</td>
    <td>ClutterDexGrasp: A Sim-to-Real System for General Dexterous Grasping in Cluttered Scenes</td>
    <td>Two-stage teacherâ€“student framework enabling zero-shot dexterous grasping in clutter.</td>
    <td><a href="https://clutterdexgrasp.github.io/">link</a></td>
    <td><a href="https://arxiv.org/abs/2506.14317">link</a></td>
    <td><a href="https://github.com/QiyangYan/ClutterDexGrasp">link</a></td>
  </tr>
  <tr>
    <td>2</td>
    <td>ScrewSplat: An End-to-End Method for Articulated Object Recognition</td>
    <td>Learns geometry and kinematics from RGB using Gaussian splatting with screw axes.</td>
    <td>N/A</td>
    <td><a href=https://arxiv.org/abs/2508.02146">link</a></td>
    <td><a href="https://github.com/seungyeon-k/ScrewSplat-public">link</a></td>
  </tr>
  <tr>
    <td>3</td>
    <td>Visual Imitation Enables Contextual Humanoid Control</td>
    <td>Large-scale visual imitation enables humanoids to act contextually in varied environments.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.03729">link</a></td>
    <td><a href="https://github.com/hongsukchoi/VideoMimic">link</a></td>
  </tr>
  <tr>
    <td>4</td>
    <td>Sampling-based System Identification with Active Exploration for Legged Sim2Real Learning</td>
    <td>Active exploration improves system ID and sim-to-real transfer for legged robots.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.14266">link</a></td>
    <td><a href="https://github.com/LeCAR-Lab/SPI-Active">link</a></td>
  </tr>
  <tr>
    <td>5</td>
    <td>DemoSpeedup: Accelerating Visuomotor Policies via Entropy-Guided Demonstration Acceleration</td>
    <td>Speeds up visuomotor policy training by entropy-guided demo selection.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2506.05064">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>6</td>
    <td>RoboArena: Distributed Real-World Evaluation of Generalist Robot Policies</td>
    <td>Distributed real-world benchmark for evaluating generalist robot policies.</td>
    <td><a href="https://roboarena.org">link</a></td>
    <td><a href="https://arxiv.org/abs/2506.18123">link</a></td>
    <td><a href="https://github.com/RoboArena/roboarena">link</a></td>
  </tr>
  <tr>
    <td>7</td>
    <td>Latent Theory of Mind: A Decentralized Diffusion Architecture for Cooperative Manipulation</td>
    <td>Multi-agent cooperative manipulation via diffusion-based latent theory of mind.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.09144">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>8</td>
    <td>Non-conflicting Energy Minimization in Reinforcement Learning based Robot Control</td>
    <td>Energy-efficient robot control via non-conflicting energy minimization in RL.</td>
    <td>N/A</td>
    <td><a href="https://www.arxiv.org/abs/2509.01765">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>9</td>
    <td>Belief-Conditioned One-Step Diffusion: Real-Time Trajectory Planning with Just-Enough Sensing</td>
    <td>Diffusion planner for real-time trajectories under partial observability.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2508.12166">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>10</td>
    <td>Cross-Sensor Touch Generation</td>
    <td>Generates cross-sensor tactile signals to improve robot perception.</td>
    <td><a href="https://samantabelen.github.io/cross_sensor_touch_generation/">link</a></td>
    <td><a href="https://openreview.net/forum?id=oGcC8nMOit#discussion">OpenReview</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>11</td>
    <td>X-Sim: Cross-Embodiment Learning via Real-to-Sim-to-Real</td>
    <td>Real-to-sim-to-real pipeline for transferring skills across robot embodiments.</td>
    <td><a href="https://portal-cornell.github.io/X-Sim/">link</a></td>
    <td><a href="https://arxiv.org/abs/2505.07096">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>12</td>
    <td>Data Retrieval with Importance Weights for Few-Shot Imitation Learning</td>
    <td>Improves few-shot imitation learning by weighting retrieved demos for efficiency.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2509.01657">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>13</td>
    <td>Versatile Loco-Manipulation through Flexible Interlimb Coordination</td>
    <td>Enables loco-manipulation by flexibly coordinating robot limbs.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2506.07876">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>14</td>
    <td>Reactive In-Air Clothing Manipulation with Confidence-Aware Dense Correspondence and Visuotactile Affordance</td>
    <td>In-air garment manipulation using dense visual-tactile correspondences and affordances.</td>
    <td>N/A</td>
    <td><a href="https://openreview.net/attachment?id=sXoaNAECCK&name=pdf">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>15</td>
    <td>Training Strategies for Efficient Embodied Reasoning</td>
    <td>Proposes strategies to make embodied reasoning more efficient.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.08243">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>16</td>
    <td>Divide, Discover, Deploy: Factorized Skill Learning with Symmetry and Style Priors</td>
    <td>Factorized skill learning with priors for modular deployment.</td>
    <td><a href="https://leggedrobotics.github.io/d3-skill-discovery/">link</a></td>
    <td><a href="https://arxiv.org/abs/2506.07482">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>17</td>
    <td>Tactile Beyond Pixels: Multisensory Touch Representations for Robot Manipulation</td>
    <td>Builds rich tactile representations that fuse multiple modalities.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2506.14754">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>18</td>
    <td>Planning from Point Clouds over Continuous Actions for Multi-object Rearrangement</td>
    <td>Planner for rearrangement tasks directly from raw point clouds.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2509.04645">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>19</td>
    <td>Stack It Up!: 3D Stable Structure Generation from 2D Hand-drawn Sketch</td>
    <td>Converts 2D sketches into physically stable 3D structures.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2508.02093">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>20</td>
    <td>DexSkin: High-Coverage Conformable Robotic Skin for Learning Contact-Rich Manipulation</td>
    <td>Flexible, conformable robotic skin for learning contact-rich tasks.</td>
    <td>N/A</td>
    <td><a href="https://www.arxiv.org/abs/2509.18830">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>21</td>
    <td>SAVOR: Skill Affordance Learning from Visuo-Haptic Perception for Robot-Assisted Bite Acquisition</td>
    <td>Learns visuo-haptic affordances for assistive bite acquisition.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2506.02353">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>22</td>
    <td>SAIL: Faster-than-Demonstration Execution of Imitation Learning Policies</td>
    <td>Executes imitation learning policies faster than human demonstrations.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2506.11948">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>23</td>
    <td>Geometric Red-Teaming for Robotic Manipulation</td>
    <td>Stress-tests robot manipulation with geometric adversaries.</td>
    <td>N/A</td>
    <td><a href="https://www.arxiv.org/abs/2509.12379">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>24</td>
    <td>HuB: Learning Extreme Humanoid Balance</td>
    <td>Trains humanoids for extreme balancing and recovery.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.07294">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>25</td>
    <td>Vision-Language-Action Model with Open-World Generalization</td>
    <td>VLA model trained to generalize to open-world robotic tasks.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2504.16054">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>26</td>
    <td>Fabrica: Dual-Arm Assembly of General Multi-Part Objects via Integrated Planning and Learning</td>
    <td>Combines planning and learning for generalizable dual-arm multi-part assembly.</td>
    <td><a href="">link</a></td>
    <td><a href="https://arxiv.org/abs/2506.05168">link</a></td>
    <td><a href="">link</a></td>
  </tr>
  <tr>
    <td>27</td>
    <td>Streaming Flow Policy: Simplifying diffusion/flow-matching policies by treating action trajectories as flow trajectories</td>
    <td>Simplifies diffusion policies by reformulating action trajectories as flow trajectories.</td>
    <td><a href="">link</a></td>
    <td><a href="https://arxiv.org/abs/2505.21851">link</a></td>
    <td><a href="">link</a></td>
  </tr>
  <tr>
    <td>28</td>
    <td>Steering Your Diffusion Policy with Latent Space Reinforcement Learning</td>
    <td>Uses reinforcement learning in latent space to guide and improve diffusion policies.</td>
    <td><a href="https://diffusion-steering.github.io/">link</a></td>
    <td><a href="https://arxiv.org/abs/2506.15799">link</a></td>
    <td><a href="https://github.com/nakamotoo/dsrl_pi0">link</a></td>
  </tr>
  <tr>
    <td>29</td>
    <td>AirExo-2: Scaling up Generalizable Robotic Imitation Learning with Low-Cost Exoskeletons</td>
    <td>Scales robotic imitation learning using low-cost wearable exoskeleton data collection.</td>
    <td><a href="https://airexo.tech/airexo2/">link</a></td>
    <td><a href="https://arxiv.org/pdf/2503.03081">link</a></td>
    <td><a href="">link</a></td>
  </tr>
  <tr>
    <td>30</td>
    <td>FetchBot: Learning Generalizable Object Fetching in Cluttered Scenes via Zero-Shot Sim2Real</td>
    <td>Zero-shot sim-to-real transfer for object fetching in cluttered environments.</td>
    <td><a href="https://pku-epic.github.io/FetchBot/">link</a></td>
    <td><a href="https://arxiv.org/abs/2502.17894">link</a></td>
    <td><a href="">link</a></td>
  </tr>
  <tr>
    <td>31</td>
    <td>One View, Many Worlds: Single-Image to 3D Object Meets Generative Domain Randomization for One-Shot 6D Pose Estimation</td>
    <td>One-shot 6D pose estimation using single-image 3D object generation with domain randomization.</td>
    <td><a href="">link</a></td>
    <td><a href="https://arxiv.org/abs/2509.07978">link</a></td>
    <td><a href="https://github.com/GZWSAMA/OnePoseviaGen">link</a></td>
  </tr>
  <tr>
    <td>32</td>
    <td>KineSoft: Learning Proprioceptive Manipulation Policies with Soft Robot Hands</td>
    <td>Trains manipulation policies for soft robot hands using proprioceptive sensing.</td>
    <td><a href="">link</a></td>
    <td><a href="https://arxiv.org/abs/2503.01078">link</a></td>
    <td><a href="">link</a></td>
  </tr>
  <tr>
    <td>33</td>
    <td>Real2Render2Real: Scaling Robot Data Without Dynamics Simulation or Robot Hardware</td>
    <td>Data scaling pipeline using rendering to bypass robot hardware and dynamics simulation.</td>
    <td><a href="">link</a></td>
    <td><a href="https://arxiv.org/abs/2505.09601">link</a></td>
    <td><a href="https://github.com/BerkeleyAutomation/Real2Render2Real">link</a></td>
  </tr>
  <tr>
    <td>34</td>
    <td>FACET: Force-Adaptive Control via Impedance Reference Tracking for Legged Robots</td>
    <td>Force-adaptive impedance tracking improves control of legged robots.</td>
    <td><a href="">link</a></td>
    <td><a href="https://arxiv.org/abs/2505.06883">link</a></td>
    <td><a href="">link</a></td>
  </tr>
  <tr>
    <td>35</td>
    <td>DexUMI: Using Human Hand as the Universal Manipulation Interface for Dexterous Manipulation</td>
    <td>Leverages the human hand as a universal interface for dexterous robotic manipulation.</td>
    <td><a href="">link</a></td>
    <td><a href="https://arxiv.org/abs/2505.21864">link</a></td>
    <td><a href="https://github.com/columbia-ai-robotics/dexumi">link</a></td>
  </tr>
  <tr>
    <td>36</td>
    <td>The Sound of Simulation: Learning Multimodal Sim-to-Real Robot Policies with Generative Audio</td>
    <td>Integrates audio with vision for multimodal sim-to-real robot policies.</td>
    <td><a href="https://renhao-wang.github.io/sound-of-sim/">link</a></td>
    <td><a href="https://arxiv.org/abs/2507.02864">link</a></td>
    <td><a href="">link</a></td>
  </tr>
  <tr>
    <td>37</td>
    <td>ReWiND: Language-Guided Rewards Teach Robot Policies without New Demonstrations</td>
    <td>Uses language-guided rewards to improve robot policies without requiring new demos.</td>
    <td><a href="">link</a></td>
    <td><a href="https://arxiv.org/abs/2505.10911">link</a></td>
    <td><a href="">link</a></td>
  </tr>
  <tr>
    <td>38</td>
    <td>Learning a Unified Policy for Position and Force Control in Legged Loco-Manipulation</td>
    <td>Unified policy framework for position and force control in legged loco-manipulation.</td>
    <td><a href="">link</a></td>
    <td><a href="https://arxiv.org/abs/2505.20829">link</a></td>
    <td><a href="">link</a></td>
  </tr>
  <tr>
    <td>39</td>
    <td>Omni-Perception: Omnidirectional Collision Avoidance of Legged Robots in Dynamic Environments</td>
    <td>Omnidirectional perception enables collision avoidance for legged robots in dynamic settings.</td>
    <td><a href="">link</a></td>
    <td><a href="https://arxiv.org/abs/2505.19214">link</a></td>
    <td><a href="">link</a></td>
  </tr>
  <tr>
    <td>40</td>
    <td>Real-Time Out-of-Distribution Failure Prevention via Multi-Modal Reasoning</td>
    <td>Prevents OOD failures in real-time with multi-modal reasoning.</td>
    <td><a href="">link</a></td>
    <td><a href="https://arxiv.org/abs/2505.10547">link</a></td>
    <td><a href="">link</a></td>
  </tr>
  <tr>
    <td>41</td>
    <td>LocoFormer: Generalist Locomotion via Long-context Adaptation</td>
    <td>Generalist locomotion policy trained with long-context sequence adaptation.</td>
    <td><a href="https://generalist-locomotion.github.io/">link</a></td>
    <td><a href="https://openreview.net/pdf/09f1069aefe99e01a46eef1033e2dbd007f687bd.pdf">link</a></td>
    <td><a href="https://github.com/minliu-cs/locformer">link</a></td>
  </tr>
  <tr>
    <td>42</td>
    <td>ImMimic: Cross-Domain Imitation from Human Videos via Mapping and Interpolation</td>
    <td>Cross-domain imitation by mapping and interpolating human videos for robot learning.</td>
    <td><a href="">link</a></td>
    <td><a href="https://arxiv.org/abs/2509.10952">link</a></td>
    <td><a href="">link</a></td>
  </tr>
</table>

# CoRL 2025 Papers
A list of 200+ [CoRL](https://www.corl.org/) papers (orals & posters) with TLDRs, project pages, code, etc

<img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcT2ECLXoUBNvovVJlOH8A35HG-fKzB_b2OycA&s" alt="CORL Logo">

## Orals

<table>
  <tr>
    <th>#</th>
    <th>Paper Title</th>
    <th>TLDR</th>
    <th>Project Page</th>
    <th>Paper</th>
    <th>Code</th>
  </tr>
  <tr>
    <td>1</td>
    <td>ClutterDexGrasp: A Sim-to-Real System for General Dexterous Grasping in Cluttered Scenes</td>
    <td>Two-stage teacher–student framework enabling zero-shot dexterous grasping in clutter.</td>
    <td><a href="https://clutterdexgrasp.github.io/">link</a></td>
    <td><a href="https://arxiv.org/abs/2506.14317">link</a></td>
    <td><a href="https://github.com/QiyangYan/ClutterDexGrasp">link</a></td>
  </tr>
  <tr>
    <td>2</td>
    <td>ScrewSplat: An End-to-End Method for Articulated Object Recognition</td>
    <td>Learns geometry and kinematics from RGB using Gaussian splatting with screw axes.</td>
    <td>N/A</td>
    <td><a href=https://arxiv.org/abs/2508.02146">link</a></td>
    <td><a href="https://github.com/seungyeon-k/ScrewSplat-public">link</a></td>
  </tr>
  <tr>
    <td>3</td>
    <td>Visual Imitation Enables Contextual Humanoid Control</td>
    <td>Large-scale visual imitation enables humanoids to act contextually in varied environments.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.03729">link</a></td>
    <td><a href="https://github.com/hongsukchoi/VideoMimic">link</a></td>
  </tr>
  <tr>
    <td>4</td>
    <td>Sampling-based System Identification with Active Exploration for Legged Sim2Real Learning</td>
    <td>Active exploration improves system ID and sim-to-real transfer for legged robots.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.14266">link</a></td>
    <td><a href="https://github.com/LeCAR-Lab/SPI-Active">link</a></td>
  </tr>
  <tr>
    <td>5</td>
    <td>DemoSpeedup: Accelerating Visuomotor Policies via Entropy-Guided Demonstration Acceleration</td>
    <td>Speeds up visuomotor policy training by entropy-guided demo selection.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2506.05064">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>6</td>
    <td>RoboArena: Distributed Real-World Evaluation of Generalist Robot Policies</td>
    <td>Distributed real-world benchmark for evaluating generalist robot policies.</td>
    <td><a href="https://roboarena.org">link</a></td>
    <td><a href="https://arxiv.org/abs/2506.18123">link</a></td>
    <td><a href="https://github.com/RoboArena/roboarena">link</a></td>
  </tr>
  <tr>
    <td>7</td>
    <td>Latent Theory of Mind: A Decentralized Diffusion Architecture for Cooperative Manipulation</td>
    <td>Multi-agent cooperative manipulation via diffusion-based latent theory of mind.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.09144">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>8</td>
    <td>Non-conflicting Energy Minimization in Reinforcement Learning based Robot Control</td>
    <td>Energy-efficient robot control via non-conflicting energy minimization in RL.</td>
    <td>N/A</td>
    <td><a href="https://www.arxiv.org/abs/2509.01765">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>9</td>
    <td>Belief-Conditioned One-Step Diffusion: Real-Time Trajectory Planning with Just-Enough Sensing</td>
    <td>Diffusion planner for real-time trajectories under partial observability.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2508.12166">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>10</td>
    <td>Cross-Sensor Touch Generation</td>
    <td>Generates cross-sensor tactile signals to improve robot perception.</td>
    <td><a href="https://samantabelen.github.io/cross_sensor_touch_generation/">link</a></td>
    <td><a href="https://openreview.net/forum?id=oGcC8nMOit#discussion">OpenReview</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>11</td>
    <td>X-Sim: Cross-Embodiment Learning via Real-to-Sim-to-Real</td>
    <td>Real-to-sim-to-real pipeline for transferring skills across robot embodiments.</td>
    <td><a href="https://portal-cornell.github.io/X-Sim/">link</a></td>
    <td><a href="https://arxiv.org/abs/2505.07096">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>12</td>
    <td>Data Retrieval with Importance Weights for Few-Shot Imitation Learning</td>
    <td>Improves few-shot imitation learning by weighting retrieved demos for efficiency.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2509.01657">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>13</td>
    <td>Versatile Loco-Manipulation through Flexible Interlimb Coordination</td>
    <td>Enables loco-manipulation by flexibly coordinating robot limbs.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2506.07876">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>14</td>
    <td>Reactive In-Air Clothing Manipulation with Confidence-Aware Dense Correspondence and Visuotactile Affordance</td>
    <td>In-air garment manipulation using dense visual-tactile correspondences and affordances.</td>
    <td>N/A</td>
    <td><a href="https://openreview.net/attachment?id=sXoaNAECCK&name=pdf">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>15</td>
    <td>Training Strategies for Efficient Embodied Reasoning</td>
    <td>Proposes strategies to make embodied reasoning more efficient.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.08243">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>16</td>
    <td>Divide, Discover, Deploy: Factorized Skill Learning with Symmetry and Style Priors</td>
    <td>Factorized skill learning with priors for modular deployment.</td>
    <td><a href="https://leggedrobotics.github.io/d3-skill-discovery/">link</a></td>
    <td><a href="https://arxiv.org/abs/2506.07482">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>17</td>
    <td>Tactile Beyond Pixels: Multisensory Touch Representations for Robot Manipulation</td>
    <td>Builds rich tactile representations that fuse multiple modalities.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2506.14754">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>18</td>
    <td>Planning from Point Clouds over Continuous Actions for Multi-object Rearrangement</td>
    <td>Planner for rearrangement tasks directly from raw point clouds.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2509.04645">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>19</td>
    <td>Stack It Up!: 3D Stable Structure Generation from 2D Hand-drawn Sketch</td>
    <td>Converts 2D sketches into physically stable 3D structures.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2508.02093">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>20</td>
    <td>DexSkin: High-Coverage Conformable Robotic Skin for Learning Contact-Rich Manipulation</td>
    <td>Flexible, conformable robotic skin for learning contact-rich tasks.</td>
    <td>N/A</td>
    <td><a href="https://www.arxiv.org/abs/2509.18830">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>21</td>
    <td>SAVOR: Skill Affordance Learning from Visuo-Haptic Perception for Robot-Assisted Bite Acquisition</td>
    <td>Learns visuo-haptic affordances for assistive bite acquisition.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2506.02353">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>22</td>
    <td>SAIL: Faster-than-Demonstration Execution of Imitation Learning Policies</td>
    <td>Executes imitation learning policies faster than human demonstrations.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2506.11948">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>23</td>
    <td>Geometric Red-Teaming for Robotic Manipulation</td>
    <td>Stress-tests robot manipulation with geometric adversaries.</td>
    <td>N/A</td>
    <td><a href="https://www.arxiv.org/abs/2509.12379">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>24</td>
    <td>HuB: Learning Extreme Humanoid Balance</td>
    <td>Trains humanoids for extreme balancing and recovery.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.07294">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>25</td>
    <td>Vision-Language-Action Model with Open-World Generalization</td>
    <td>VLA model trained to generalize to open-world robotic tasks.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2504.16054">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>26</td>
    <td>Fabrica: Dual-Arm Assembly of General Multi-Part Objects via Integrated Planning and Learning</td>
    <td>Combines planning and learning for generalizable dual-arm multi-part assembly.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2506.05168">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>27</td>
    <td>Streaming Flow Policy: Simplifying diffusion/flow-matching policies by treating action trajectories as flow trajectories</td>
    <td>Simplifies diffusion policies by reformulating action trajectories as flow trajectories.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.21851">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>28</td>
    <td>Steering Your Diffusion Policy with Latent Space Reinforcement Learning</td>
    <td>Uses reinforcement learning in latent space to guide and improve diffusion policies.</td>
    <td><a href="https://diffusion-steering.github.io/">link</a></td>
    <td><a href="https://arxiv.org/abs/2506.15799">link</a></td>
    <td><a href="https://github.com/nakamotoo/dsrl_pi0">link</a></td>
  </tr>
  <tr>
    <td>29</td>
    <td>AirExo-2: Scaling up Generalizable Robotic Imitation Learning with Low-Cost Exoskeletons</td>
    <td>Scales robotic imitation learning using low-cost wearable exoskeleton data collection.</td>
    <td><a href="https://airexo.tech/airexo2/">link</a></td>
    <td><a href="https://arxiv.org/pdf/2503.03081">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>30</td>
    <td>FetchBot: Learning Generalizable Object Fetching in Cluttered Scenes via Zero-Shot Sim2Real</td>
    <td>Zero-shot sim-to-real transfer for object fetching in cluttered environments.</td>
    <td><a href="https://pku-epic.github.io/FetchBot/">link</a></td>
    <td><a href="https://arxiv.org/abs/2502.17894">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>31</td>
    <td>One View, Many Worlds: Single-Image to 3D Object Meets Generative Domain Randomization for One-Shot 6D Pose Estimation</td>
    <td>One-shot 6D pose estimation using single-image 3D object generation with domain randomization.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2509.07978">link</a></td>
    <td><a href="https://github.com/GZWSAMA/OnePoseviaGen">link</a></td>
  </tr>
  <tr>
    <td>32</td>
    <td>KineSoft: Learning Proprioceptive Manipulation Policies with Soft Robot Hands</td>
    <td>Trains manipulation policies for soft robot hands using proprioceptive sensing.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2503.01078">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>33</td>
    <td>Real2Render2Real: Scaling Robot Data Without Dynamics Simulation or Robot Hardware</td>
    <td>Data scaling pipeline using rendering to bypass robot hardware and dynamics simulation.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.09601">link</a></td>
    <td><a href="https://github.com/BerkeleyAutomation/Real2Render2Real">link</a></td>
  </tr>
  <tr>
    <td>34</td>
    <td>FACET: Force-Adaptive Control via Impedance Reference Tracking for Legged Robots</td>
    <td>Force-adaptive impedance tracking improves control of legged robots.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.06883">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>35</td>
    <td>DexUMI: Using Human Hand as the Universal Manipulation Interface for Dexterous Manipulation</td>
    <td>Leverages the human hand as a universal interface for dexterous robotic manipulation.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.21864">link</a></td>
    <td><a href="https://github.com/columbia-ai-robotics/dexumi">link</a></td>
  </tr>
  <tr>
    <td>36</td>
    <td>The Sound of Simulation: Learning Multimodal Sim-to-Real Robot Policies with Generative Audio</td>
    <td>Integrates audio with vision for multimodal sim-to-real robot policies.</td>
    <td><a href="https://renhao-wang.github.io/sound-of-sim/">link</a></td>
    <td><a href="https://arxiv.org/abs/2507.02864">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>37</td>
    <td>ReWiND: Language-Guided Rewards Teach Robot Policies without New Demonstrations</td>
    <td>Uses language-guided rewards to improve robot policies without requiring new demos.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.10911">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>38</td>
    <td>Learning a Unified Policy for Position and Force Control in Legged Loco-Manipulation</td>
    <td>Unified policy framework for position and force control in legged loco-manipulation.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.20829">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>39</td>
    <td>Omni-Perception: Omnidirectional Collision Avoidance of Legged Robots in Dynamic Environments</td>
    <td>Omnidirectional perception enables collision avoidance for legged robots in dynamic settings.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.19214">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>40</td>
    <td>Real-Time Out-of-Distribution Failure Prevention via Multi-Modal Reasoning</td>
    <td>Prevents OOD failures in real-time with multi-modal reasoning.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.10547">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>41</td>
    <td>LocoFormer: Generalist Locomotion via Long-context Adaptation</td>
    <td>Generalist locomotion policy trained with long-context sequence adaptation.</td>
    <td><a href="https://generalist-locomotion.github.io/">link</a></td>
    <td><a href="https://openreview.net/pdf/09f1069aefe99e01a46eef1033e2dbd007f687bd.pdf">link</a></td>
    <td><a href="https://github.com/minliu-cs/locformer">link</a></td>
  </tr>
  <tr>
    <td>42</td>
    <td>ImMimic: Cross-Domain Imitation from Human Videos via Mapping and Interpolation</td>
    <td>Cross-domain imitation by mapping and interpolating human videos for robot learning.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2509.10952">link</a></td>
    <td>N/A</td>
  </tr>
</table>

## Posters
<table>
  <tr>
    <th>#</th>
    <th>Paper Title</th>
    <th>TLDR</th>
    <th>Project Page</th>
    <th>Paper</th>
    <th>Code</th>
  </tr>
  <tr>
    <td>1</td>
    <td>CLONE: Closed-Loop Whole-Body Humanoid Teleoperation for Long-Horizon Tasks</td>
    <td>Robust whole-body humanoid teleoperation using mixture-of-experts control and LiDAR correction.</td>
    <td><a href="https://humanoid-clone.github.io/">link</a></td>
    <td><a href="https://arxiv.org/abs/2506.08931">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>2</td>
    <td>Disentangled Multi-Context Meta-Learning: Unlocking Robust and Generalized Task Learning</td>
    <td>Disentangles task factors for robust meta-learning and sim-to-real transfer.</td>
    <td>N/A</td>
    <td><a href="https://www.arxiv.org/abs/2509.01297">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>3</td>
    <td>Meta-Optimization and Program Search using Language Models for Task and Motion Planning</td>
    <td>Uses LLMs for meta-optimization and program search in task and motion planning.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.03725">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>4</td>
    <td>Text2Touch: Tactile In-Hand Manipulation with LLM-Designed Reward Functions</td>
    <td>Leverages LLM-designed reward functions for tactile in-hand robot manipulation.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/html/2509.07445v1">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>5</td>
    <td>Multi-Loco: Unifying Multi-Embodiment Legged Locomotion via Reinforcement Learning Augmented Diffusion</td>
    <td>Unifies locomotion control across embodiments with RL-augmented diffusion models.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2506.11470">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>6</td>
    <td>SimShear: Sim-to-Real Shear-based Tactile Servoing</td>
    <td>Uses shear-based tactile feedback for sim-to-real transfer in tactile servoing.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2508.20561">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>7</td>
    <td>Focusing on What Matters: Object-Agent-centric Tokenization for Vision Language Action models</td>
    <td>Introduces object-agent-centric tokenization to improve VLA model reasoning.</td>
    <td>N/A</td>
    <td><a href="https://openreview.net/attachment?id=Ict1OjU9gl&name=pdf">OpenReview</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>8</td>
    <td>AT-Drone: Benchmarking Adaptive Teaming in Multi-Drone Pursuit</td>
    <td>Benchmark for evaluating adaptive teaming strategies in multi-drone pursuit tasks.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2502.09762">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>9</td>
    <td>Uncertainty-Aware Scene Understanding via Efficient Sampling-Free Confidence Estimation</td>
    <td>Confidence estimation for scene understanding without expensive sampling.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/html/2411.11935v2">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>10</td>
    <td>ObjectReact: Learning Object-Relative Control for Visual Navigation</td>
    <td>Learns object-relative navigation control for robust visual navigation.</td>
    <td><a href="https://object-react.github.io/">Link</a></td>
    <td><a href="https://arxiv.org/abs/2509.09594">link</a></td>
    <td><a href="https://github.com/oravus/object-rel-nav">Link</a></td>
  </tr>
  <tr>
    <td>11</td>
    <td>Decentralized Aerial Manipulation of a Cable-Suspended Load Using Multi-Agent Reinforcement Learning</td>
    <td>Decentralized multi-agent RL for aerial manipulation of suspended loads.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2508.01522">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>12</td>
    <td>ReasonPlan: Unified Scene Prediction and Decision Reasoning for Closed-loop Autonomous Driving</td>
    <td>Integrates scene prediction and decision reasoning for end-to-end autonomous driving.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.20024">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>13</td>
    <td>Embrace Contacts: humanoid shadowing with full body ground contacts</td>
    <td>Enables humanoid shadowing with consistent full-body ground contacts.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2502.01465">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>14</td>
    <td>Distilling for Long-Horizon Prehensile and Non-Prehensile Manipulation</td>
    <td>Distills policies to handle long-horizon prehensile and non-prehensile tasks.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2502.18015">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>15</td>
    <td>Constraint-Aware Diffusion Guidance for Robotics: Real-Time Obstacle Avoidance for Autonomous Racing</td>
    <td>Uses diffusion guidance for real-time obstacle avoidance in racing scenarios.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.13131">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>16</td>
    <td>Learning Impact-Rich Rotational Maneuvers via Centroidal Velocity Rewards and Sim-to-Real Techniques: A One-Leg Hopper Flip Case Study</td>
    <td>Trains one-leg hopper to flip using impact-rich rewards and sim-to-real techniques.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.12222">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>17</td>
    <td>LodeStar: Long-horizon Dexterity via Synthetic Data Augmentation from Human Demonstrations</td>
    <td>Enhances dexterity with synthetic data augmentation from human demonstrations.</td>
    <td><a href="https://lodestar-robot.github.io/">link</a></td>
    <td><a href="https://arxiv.org/abs/2508.17547">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>18</td>
    <td>Imitation Learning Based on Disentangled Representation Learning of Behavioral Characteristics</td>
    <td>Applies disentangled representation learning for imitation learning of behaviors.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2509.04737">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>19</td>
    <td>Motion Priors Reimagined: Adapting Flat-Terrain Skills for Complex Quadruped Mobility</td>
    <td>Adapts flat-terrain locomotion skills to complex terrains using motion priors.</td>
    <td><a href="https://motion-priors.github.io/">link</a></td>
    <td><a href="https://arxiv.org/abs/2505.16084">link</a></td>
    <td><a href="https://github.com/leggedrobotics/motionpriors">link</a></td>
  </tr>
  <tr>
    <td>20</td>
    <td>Sequence Modeling for Time-Optimal Quadrotor Trajectory Optimization with Sampling-based Robustness Analysis</td>
    <td>Applies sequence modeling for fast and robust quadrotor trajectory optimization.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2506.13915">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>21</td>
    <td>HALO: Human Preference Aligned Offline Reward Learning for Robot Navigation</td>
    <td>Aligns offline reward learning with human preferences for navigation tasks.</td>
    <td>N/A</td>
    <td><a href="https://www.arxiv.org/abs/2508.01539">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>22</td>
    <td>RoboMonkey: Scaling Test-Time Sampling and Verification for Vision-Language-Action Models</td>
    <td>Scales up verification and sampling for VLA models during test-time.</td>
    <td><a href="https://robomonkey-vla.github.io/">link</a></td>
    <td><a href="https://arxiv.org/abs/2506.17811">link</a></td>
    <td><a href="https://github.com/robomonkey-vla/RoboMonkey">link</a></td>
  </tr>
  <tr>
    <td>23</td>
    <td>Constraint-Preserving Data Generation for One-Shot Visuomotor Policy Generalization</td>
    <td>Generates constraint-preserving data to improve one-shot visuomotor generalization.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2508.03944">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>24</td>
    <td>Hand-Eye Autonomous Delivery: Learning Humanoid Navigation, Locomotion and Reaching</td>
    <td>Trains humanoid robots for navigation and reaching in delivery scenarios.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2508.03068">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>25</td>
    <td>FLARE: Robot Learning with Implicit World Modeling</td>
    <td>Implicit world modeling for more efficient and robust robot learning.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.15659">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>26</td>
    <td>From Real World to Logic and Back: Learning Generalizable Relational Concepts For Long Horizon Robot Planning</td>
    <td>Learns relational concepts that bridge real-world experience with symbolic planning for long-horizon tasks.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2402.11871">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>27</td>
    <td>NeuralSVCD for Efficient Swept Volume Collision Detection</td>
    <td>Neural approach to fast swept-volume–based collision detection for motion planning.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2509.00499">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>28</td>
    <td>MEReQ: Max-Ent Residual-Q Inverse RL for Sample-Efficient Alignment from Intervention</td>
    <td>Inverse RL method that aligns policies from sparse interventions via max-entropy residual Q-learning.</td>
    <td><a href="https://thomaschen98.github.io/mereq.github.io/">Link</a></td>
    <td><a href="https://arxiv.org/abs/2406.16258">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>29</td>
    <td>Joint Model-based Model-free Diffusion for Planning with Constraints</td>
    <td>Combines model-based and model-free diffusion to plan under explicit constraints.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2509.08775">link</a></td>
    <td><a href="https://jm2d-corl25.github.io/">Link</a></td>
  </tr>
  <tr>
    <td>30</td>
    <td>DEQ-MPC : Deep Equilibrium Model Predictive Control</td>
    <td>Uses deep equilibrium networks to solve MPC problems efficiently at inference time.</td>
    <td>N/A</td>
    <td><a href="https://openreview.net/pdf?id=Ty7xx0pn0a">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>31</td>
    <td>CUPID: Curating Data your Robot Loves with Influence Functions</td>
    <td>Applies influence functions to select training data that most benefits robot learning.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2506.19121">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>32</td>
    <td>Eye, Robot: Learning to Look to Act with a BC-RL Perception-Action Loop</td>
    <td>Trains perception to support action using a behavior-cloning + RL loop for active visual policies.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2506.10968">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>33</td>
    <td>Imagine, Verify, Execute: Memory-guided Agentic Exploration with Vision-Language Models</td>
    <td>Uses VLMs with memory to imagine plans, verify outcomes, and execute tasks agentically.</td>
    <td><a href="https://ive-robot.github.io/">link</a></td>
    <td><a href="https://ive-robot.github.io/static/full_paper.pdf">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>34</td>
    <td>Dynamics-Compliant Trajectory Diffusion for Super-Nominal Payload Manipulation</td>
    <td>Generates manipulation trajectories via diffusion that respect dynamics under heavy payloads.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2508.21375">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>35</td>
    <td>CoRI: Communication of Robot Intent for Physical Human-Robot Interaction</td>
    <td>Framework for communicating robot intent to improve safety and fluency in pHRI.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.20537">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>36</td>
    <td>KoopMotion: Learning Almost Divergence Free Koopman Flow Fields for Motion Planning</td>
    <td>Learns Koopman operator–based flow fields that guide smooth, divergence-free motion plans.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2509.09074">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>37</td>
    <td>Learning Smooth State-Dependent Traversability from Dense Point Clouds</td>
    <td>Predicts traversability as a smooth, state-dependent function directly from dense 3D point clouds.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2506.04362">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>38</td>
    <td>Leveraging Correlation Across Test Platforms for Variance-Reduced Metric Estimation</td>
    <td>Reduces evaluation variance by exploiting correlations across heterogeneous testbeds.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2506.20553">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>39</td>
    <td>Agreement Volatility: A Second-Order Metric for Uncertainty Quantification in Surgical Robot Learning</td>
    <td>Introduces a second-order metric capturing volatility in model agreement to quantify uncertainty.</td>
    <td>N/A</td>
    <td><a href="https://openreview.net/forum?id=K7KLc4FexO#discussion">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>40</td>
    <td>Sample-Efficient Online Control Policy Learning with Real-Time Recursive Model Updates</td>
    <td>Online control learning with recursive updates to maintain real-time performance.</td>
    <td>N/A</td>
    <td><a href="https://www.arxiv.org/abs/2509.08241">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>41</td>
    <td>Lucid-XR: An Extended-Reality Data Engine for Robotic Manipulation</td>
    <td>Builds an XR-based engine to synthesize diverse manipulation data at scale.</td>
    <td>N/A</td>
    <td><a href="https://openreview.net/pdf?id=3p7rTnLJM8">OpenReview</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>42</td>
    <td>CLAMP: Crowdsourcing a LArge-scale in-the-wild haptic dataset with an open-source device for Multimodal robot Perception</td>
    <td>Collects large-scale in-the-wild haptic data using an open-source crowdsourced device.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.21495">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>43</td>
    <td>Distributed Upload and Active Labeling for Resource-Constrained Fleet Learning</td>
    <td>Active labeling and distributed upload strategies tailored for fleet learning under bandwidth limits.</td>
    <td>N/A</td>
    <td><a href="https://openreview.net/pdf/e94bfc7fafbb86a2ace178388c3d571221c3ef12.pdf">OpenReview</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>44</td>
    <td>DreamGen: Unlocking Generalization in Robot Learning through Video World Models</td>
    <td>Uses video world models to improve generalization across robot tasks and environments.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.12705">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>45</td>
    <td>LocoTouch: Learning Dynamic Quadrupedal Transport with Tactile Sensing</td>
    <td>Incorporates tactile feedback for robust, dynamic quadruped transport behaviors.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.23175">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>46</td>
    <td>TopoCut: Learning Multi-Step Cutting with Spectral Rewards and Discrete Diffusion Policies</td>
    <td>Solves multi-step cutting via discrete diffusion guided by topology-aware rewards.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2509.19712">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>47</td>
    <td>WoMAP: World Models For Embodied Open-Vocabulary Object Localization</td>
    <td>Uses open-vocabulary world models to localize objects for embodied agents.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2506.01600">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>48</td>
    <td>MirrorDuo: Reflection-Consistent Visuomotor Learning from Mirrored Demonstration Pairs</td>
    <td>Improves visuomotor learning by enforcing reflection consistency across mirrored demos.</td>
    <td>N/A</td>
    <td><a href="https://openreview.net/pdf/c9df128e0cbb8f0f2d511028317205ec9c07e38c.pdf">OpenReview</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>49</td>
    <td>First Order Model-Based RL through Decoupled Backpropagation</td>
    <td>Enables efficient model-based RL via a first-order method with decoupled gradients.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2509.00215">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>50</td>
    <td>CLASS: Contrastive Learning via Action Sequence Supervision for Robot Manipulation</td>
    <td>Contrastive pretraining supervised by action sequences to enhance manipulation policies.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2508.01600">link</a></td>
    <td>N/A</td>
  </tr>
<tr>
    <td>51</td>
    <td>Articulated Object Estimation in the Wild</td>
    <td>Estimates articulation and pose of everyday objects from unconstrained, real-world data.</td>
    <td>N/A</td>
    <td><a href="https://www.arxiv.org/abs/2509.01708">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>52</td>
    <td>Multimodal Fused Learning for Solving the Generalized Traveling Salesman Problem in Robotic Task Planning</td>
    <td>Fuses multiple sensing/modalities to solve generalized TSP instances for robot task planning.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2506.16931">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>53</td>
    <td>Dexplore: Scalable Neural Control for Dexterous Manipulation from Reference Scoped Exploration</td>
    <td>Unified optimization uses MoCap as soft guidance to scale dexterous manipulation control.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2509.09671">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>54</td>
    <td>PrioriTouch: Adapting to User Contact Preferences for Whole-Arm Physical Human-Robot Interaction</td>
    <td>Learns and prioritizes user-specific contact preferences for safer whole-arm pHRI.</td>
    <td><a href="https://emprise.cs.cornell.edu/prioritouch//">link</a></td>
    <td><a href="https://arxiv.org/abs/2509.18447">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>55</td>
    <td>UnPose: Uncertainty-Guided Diffusion Priors for Zero-Shot Pose Estimation</td>
    <td>Uses diffusion priors with uncertainty to enable model-free, zero-shot 6D pose estimation.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2508.15972">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>56</td>
    <td>Distilling On-device Language Models for Robot Planning with Minimal Human Intervention</td>
    <td>Distills compact LMs on-device to plan robot actions with minimal human supervision.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2506.17486">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>57</td>
    <td>Mechanistic Interpretability for Steering Vision-Language-Action Models</td>
    <td>Probes and steers VLA internals using mech-int tools to improve safety and control.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2509.00328">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>58</td>
    <td>FFHFlow: Diverse and Uncertainty-Aware Dexterous Grasp Generation via Flow Variational Inference</td>
    <td>Generates diverse, uncertainty-aware dexterous grasps using flow-based variational inference.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2407.15161">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>59</td>
    <td>Tool-as-Interface: Learning Robot Policies from Observing Human Tool Use</td>
    <td>Transfers tool-use knowledge from human demonstrations to robot visuomotor policies.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2504.04612">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>60</td>
    <td>DiWA: Diffusion Policy Adaptation with World Models</td>
    <td>Offline fine-tuning of diffusion policies using a learned world model instead of real interactions.</td>
    <td><a href="https://diwa.cs.uni-freiburg.de/">link</a></td>
    <td><a href="https://arxiv.org/abs/2508.03645">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>61</td>
    <td>Self-supervised Learning Of Visual Pose Estimation Without Pose Labels By Classifying LED States</td>
    <td>Replaces pose labels with LED-state classification to self-supervise pose estimation.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/html/2509.10405v1">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>62</td>
    <td>Diffusion-Guided Multi-Arm Motion Planning</td>
    <td>Guides multi-arm planning with diffusion priors to handle high-dimensional coordination.</td>
    <td>N/A</td>
    <td><a href="https://www.arxiv.org/abs/2509.08160">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>63</td>
    <td>GraphEQA: Using 3D Semantic Scene Graphs for Real-time Embodied Question Answering</td>
    <td>Employs online 3D semantic scene graphs to plan, explore, and answer embodied questions.</td>
    <td><a href="https://saumyasaxena.github.io/grapheqa/">link</a></td>
    <td><a href="https://arxiv.org/abs/2412.14480">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>64</td>
    <td>Enter the Mind Palace: Reasoning and Planning for Long-term Active Embodied Question Answering</td>
    <td>Structured memory (“mind palace”) for long-term EQA with exploration–recall tradeoffs.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2507.12846">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>65</td>
    <td>Steerable Scene Generation with Post Training and Inference-Time Search</td>
    <td>Steers generative scene models via post-training objectives and search during inference.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.04831">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>66</td>
    <td>RICL: Adding In-Context Adaptability to Pre-Trained Vision-Language-Action Models</td>
    <td>Enables in-context task adaptation for pre-trained VLA models without full finetuning.</td>
    <td><a href="https://ricl-vla.github.io/">Link</a></td>
    <td><a href="https://arxiv.org/abs/2508.02062">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>67</td>
    <td>ARCH: Hierarchical Hybrid Learning for Long-Horizon Contact-Rich Robotic Assembly</td>
    <td>Hierarchical composition of parametric skills plus high-level policy for contact-rich assembly.</td>
    <td><a href="https://long-horizon-assembly.github.io/">link</a></td>
    <td><a href="https://arxiv.org/abs/2409.16451">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>68</td>
    <td>Pointing3D: A Benchmark for 3D Object Referral via Pointing Gestures</td>
    <td>Benchmark for grounding 3D object references expressed via human pointing gestures.</td>
    <td>N/A</td>
    <td><a href="https://openreview.net/forum?id=h2K52fhsDU#discussion">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>69</td>
    <td>Merging and Disentangling Views in Visual Reinforcement Learning for Robotic Manipulation</td>
    <td>Improves manipulation RL by merging/disentangling multi-view representations.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.04619">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>70</td>
    <td>Learning Deployable Locomotion Control via Differentiable Simulation</td>
    <td>Trains locomotion controllers in differentiable simulation for real-world deployment.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2404.02887">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>71</td>
    <td>BEHAVIOR Robot Suite: Streamlining Real-World Whole-Body Manipulation for Everyday Household Activities</td>
    <td>Open robot suite + algorithms for bimanual, mobile whole-body household manipulation.</td>
    <td><a href="https://behavior-robot-suite.github.io/">link</a></td>
    <td><a href="https://arxiv.org/abs/2503.05652">link</a></td>
    <td><a href="https://github.com/behavior-robot-suite/brs-algo">link</a></td>
  </tr>
  <tr>
    <td>72</td>
    <td>GraspQP: Differentiable Optimization of Force Closure for Diverse and Robust Dexterous Grasping</td>
    <td>Differentiable force-closure QP yields diverse, stable dexterous grasps and dataset.</td>
    <td><a href="https://graspqp.github.io/">link</a></td>
    <td><a href="https://arxiv.org/abs/2508.15002">link</a></td>
    <td><a href="https://graspqp.github.io/">link</a></td>
  </tr>
  <tr>
    <td>73</td>
    <td>Adapting by Analogy: OOD Generalization of Visuomotor Policies via Functional Correspondence</td>
    <td>Improves OOD generalization by mapping task analogies via functional correspondence.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2506.12678">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>74</td>
    <td>Long Range Navigator (LRN): Extending robot planning horizons beyond metric maps</td>
    <td>Learns affordance frontiers from vision to plan over horizons far beyond local maps.</td>
    <td><a href="https://personalrobotics.github.io/lrn/">link</a></td>
    <td><a href="https://openreview.net/forum?id=JbwHcSg2ZF">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>75</td>
    <td>Toward Real-World Cooperative and Competitive Soccer with Quadrupedal Robot Teams</td>
    <td>Hierarchical MARL for decentralized quadruped robot soccer in real-world settings.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.13834">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>76</td>
    <td>Multi-critic Learning for Whole-body End-effector Twist Tracking</td>
    <td>Uses multiple critics to improve whole-body control for accurate end-effector twist tracking.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2507.08656">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>77</td>
    <td>SIREN: Semantic, Initialization-Free Registration of Multi-Robot Gaussian Splatting Maps</td>
    <td>Registers multi-robot Gaussian splat maps using semantics without requiring initialization.</td>
    <td><a href="https://siren-robot.github.io/">Link</a></td>
    <td><a href="https://arxiv.org/abs/2502.06519">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>78</td>
    <td>Rapid Mismatch Estimation via Neural Network Informed Variational Inference</td>
    <td>Estimates model–reality mismatch quickly using VI guided by neural networks.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2508.21007">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>79</td>
    <td>In-Context Iterative Policy Improvement for Dynamic Manipulation</td>
    <td>Iteratively improves manipulation policies via in-context updates without full retraining.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2508.15021">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>80</td>
    <td>Cost-aware Discovery of Contextual Failures using Bayesian Active Learning</td>
    <td>Finds failure modes with a cost-aware Bayesian active learning strategy.</td>
    <td>N/A</td>
    <td><a href="https://openreview.net/forum?id=f2Y549UzM5#discussion">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>81</td>
    <td>KDPE: A Kernel Density Estimation Strategy for Diffusion Policy Trajectory Selection</td>
    <td>Selects better diffusion trajectories via kernel density estimation over candidates.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2508.10511">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>82</td>
    <td>SocialNav-SUB: Benchmarking VLMs for Scene Understanding in Social Robot Navigation</td>
    <td>Benchmarks vision-language models for socially aware navigation scene understanding.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2509.08757">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>83</td>
    <td>Generating Robot Constitutions &amp; Benchmarks for Semantic Safety</td>
    <td>Creates rule “constitutions” and benchmarks to evaluate semantic safety in robots.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2503.08663">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>84</td>
    <td>Few-Shot Neuro-Symbolic Imitation Learning for Long-Horizon Planning and Acting</td>
    <td>Combines neuro-symbolic reasoning with few-shot imitation for long-horizon tasks.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2508.21501">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>85</td>
    <td>Capability-Aware Shared Hypernetworks for Flexible Heterogeneous Multi-Robot Coordination</td>
    <td>Uses shared hypernetworks aware of robot capabilities for heterogeneous team coordination.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2501.06058">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>86</td>
    <td>AimBot: A Simple Auxiliary Visual Cue to Enhance Spatial Awareness of Visuomotor Policies</td>
    <td>Adds an auxiliary visual cue to boost spatial awareness in visuomotor policies.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2508.08113">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>87</td>
    <td>Beyond Constant Parameters: Hyper Prediction Models and HyperMPC</td>
    <td>Predicts time-varying parameters via hyper models and integrates them into MPC.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2508.06181">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>88</td>
    <td>FLOWER: Democratizing Generalist Robot Policies with Efficient Vision-Language-Flow Models</td>
    <td>Builds efficient VLF models to make generalist robot policies more accessible.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2509.04996">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>89</td>
    <td>Subteaming and Adaptive Formation Control for Coordinated Multi-Robot Navigation</td>
    <td>Dynamically forms subteams and adapts formations for coordinated navigation.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/html/2509.16412v1">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>90</td>
    <td>Force-Modulated Visual Policy for Robot-Assisted Dressing with Arm Motions</td>
    <td>Integrates force modulation into visual policies for safer robot-assisted dressing.</td>
    <td>N/A</td>
    <td><a href="https://www.arxiv.org/abs/2509.12741">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>91</td>
    <td>ComposableNav: Instruction-Following Navigation in Dynamic Environments via Composable Diffusion</td>
    <td>Composes diffusion skills to follow instructions for navigation in dynamic scenes.</td>
    <td>N/A</td>
    <td><a href="https://openreview.net/forum?id=QeP2TmCXKn">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>92</td>
    <td>Train-Once Plan-Anywhere Kinodynamic Motion Planning via Diffusion Trees</td>
    <td>Trains diffusion trees once to enable generalizable kinodynamic motion planning.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2508.21001">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>93</td>
    <td>ZipMPC: Compressed Context-Dependent MPC Cost via Imitation Learning</td>
    <td>Learns compact, context-aware MPC cost functions from demonstrations.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2507.13088">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>94</td>
    <td>EndoVLA: Dual-Phase Vision-Language-Action for Precise Autonomous Tracking in Endoscopy</td>
    <td>Two-phase VLA framework for accurate autonomous camera tracking in endoscopy.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.15206">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>95</td>
    <td>Morphologically Symmetric Reinforcement Learning for Ambidextrous Bimanual Manipulation</td>
    <td>Exploits morphological symmetry to learn ambidextrous bimanual manipulation skills.</td>
    <td>N/A</td>
    <td><a href="http://arxiv.org/abs/2505.05287">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>96</td>
    <td>Robot Operating Home Appliances by Reading User Manuals</td>
    <td>Extracts procedural knowledge from manuals to operate home appliances autonomously.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.20424">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>97</td>
    <td>CARE: Enhancing Safety of Visual Navigation through Collision Avoidance via Repulsive Estimation</td>
    <td>Improves navigation safety with a repulsive-estimation collision avoidance module.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2506.03834">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>98</td>
    <td>MimicFunc: Imitating Tool Manipulation from a Single Human Video via Functional Correspondence</td>
    <td>Imitates tool-use from a single video by aligning functional correspondences.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2508.13534">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>99</td>
    <td>CogniPlan: Uncertainty-Guided Path Planning with Conditional Generative Layout Prediction</td>
    <td>Predicts environment layouts conditionally and plans paths with uncertainty guidance.</td>
    <td><a href="https://yizhuo-wang.com/cogniplan/">Link</a></td>
    <td><a href="https://arxiv.org/abs/2508.03027">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>100</td>
    <td>Fast Flow-based Visuomotor Policies via Conditional Optimal Transport Couplings</td>
    <td>Trains fast visuomotor policies using conditional optimal transport–based flow couplings.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.01179">link</a></td>
    <td>N/A</td>
  </tr>
   <tr>
    <td>101</td>
    <td>Enabling Long(er) Horizon Imitation for Manipulation Tasks by Modeling Subgoal Transitions</td>
    <td>Improves long-horizon imitation by explicitly modeling transitions between subgoals.</td>
    <td>N/A</td>
    <td><a href="https://openreview.net/pdf/645ee22b77b3e2fc53b0fd53851cf6adf5c2a6e5.pdf">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>102</td>
    <td>Search-TTA: A Multi-Modal Test-Time Adaptation Framework for Visual Search in the Wild</td>
    <td>Performs multi-modal test-time adaptation to robustify visual search in-the-wild.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.11350">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>103</td>
    <td>Mastering Multi-Drone Volleyball through Hierarchical Co-Self-Play Reinforcement Learning</td>
    <td>Hierarchical co-self-play enables coordinated multi-drone volleyball behaviors.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.04317">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>104</td>
    <td>Wheeled Lab: Modern Sim2Real for Low-cost, Open-source Wheeled Robotics</td>
    <td>Low-cost open-source wheeled robotics platform with strong sim-to-real pipeline.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2502.07380">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>105</td>
    <td>DexVLA: Vision-Language Model with Plug-In Diffusion Expert for General Robot Control</td>
    <td>VLM enhanced with a plug-in diffusion expert for general-purpose robot control.</td>
    <td><a href="https://dex-vla.github.io/">Link</a></td>
    <td><a href="https://arxiv.org/abs/2502.05855">link</a></td>
    <td><a href="https://github.com/juruobenruo/DexVLA">Link</a></td>
  </tr>
  <tr>
    <td>106</td>
    <td>From Space to Time: Enabling Adaptive Safety with Learned Value Functions via Disturbance Recasting</td>
    <td>Recasts disturbances and learns value functions to adapt safety constraints over time.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2509.19597">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>107</td>
    <td>Mobi-: Mobilizing Your Robot Learning Policy</td>
    <td>Framework/tools to package and deploy learned robot policies across platforms (“mobilize”).</td>
    <td><a href="https://mobipi.github.io/">Link</a></td>
    <td><a href="https://arxiv.org/abs/2505.23692">link</a></td>
    <td><a href="https://github.com/yjy0625/mobipi">Link</a></td>
  </tr>
  <tr>
    <td>108</td>
    <td>Generative Visual Foresight Meets Task-Agnostic Pose Estimation in Robotic Table-top Manipulation</td>
    <td>Combines visual foresight with task-agnostic pose estimation for table-top tasks.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2509.00361">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>109</td>
    <td>Diffusion Dynamics Models with Generative State Estimation for Cloth Manipulation</td>
    <td>Diffusion-based dynamics and generative state estimation for deformable cloth manipulation.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2503.11999">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>110</td>
    <td>Contrastive Forward Prediction Reinforcement Learning for Adaptive Fault-Tolerant Legged Robots</td>
    <td>Contrastive forward prediction enables adaptive, fault-tolerant legged locomotion.</td>
    <td>N/A</td>
    <td><a href="https://openreview.net/pdf/58960442a8bb8a9b35ae33388c72ee5ff9171f76.pdf">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>111</td>
    <td>Action-Free Reasoning for Policy Generalization</td>
    <td>Encourages high-level reasoning without action supervision to improve generalization.</td>
    <td><a href="https://rad-generalization.github.io/">Link</a></td>
    <td><a href="https://arxiv.org/abs/2502.03729">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>112</td>
    <td>Learn from What We HAVE: History-Aware VErifier that Reasons about Past Interactions Online</td>
    <td>Online verifier leverages historical interactions to prevent policy failures.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2509.00271">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>113</td>
    <td>KineDex: Learning Tactile-Informed Visuomotor Policies via Kinesthetic Teaching for Dexterous Manipulation</td>
    <td>Kinesthetic teaching with tactile cues to train dexterous visuomotor policies.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.01974">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>114</td>
    <td>FlashBack: Consistency Model-Accelerated Shared Autonomy</td>
    <td>Uses consistency models to accelerate and stabilize shared autonomy.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.16892">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>115</td>
    <td>Granular loco-manipulation: Repositioning rocks through strategic sand avalanche</td>
    <td>Manipulates granular media to reposition rocks via controlled sand avalanches.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.12934">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>116</td>
    <td>D-CODA: Diffusion for Coordinated Dual-Arm Data Augmentation</td>
    <td>Generates coordinated bimanual data via diffusion for dual-arm manipulation learning.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.04860">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>117</td>
    <td>JaxRobotarium: Training and Deploying Multi-Robot Policies in 10 Minutes</td>
    <td>JAX-based framework for rapid training/deployment of multi-robot policies.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.06771">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>118</td>
    <td>Uncertainty-aware Latent Safety Filters for Avoiding Out-of-Distribution Failures</td>
    <td>Latent-space safety filters with uncertainty estimates to avoid OOD failures.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.00779">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>119</td>
    <td>ATK: Automatic Task-driven Keypoint Selection for Robust Policy Learning</td>
    <td>Selects task-relevant keypoints automatically to improve policy robustness.</td>
    <td>N/A</td>
    <td><a href="https://www.arxiv.org/abs/2506.13867">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>120</td>
    <td>ManipBench: Benchmarking Vision-Language Models for Low-Level Robot Manipulation</td>
    <td>Benchmark to test VLMs on low-level manipulation perception and control tasks.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.09698">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>121</td>
    <td>IRIS: An Immersive Robot Interaction System</td>
    <td>Immersive system for interacting with robots using mixed/extended reality interfaces.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2502.03297">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>122</td>
    <td>ActLoc: Learning to Localize on the Move via Active Viewpoint Selection</td>
    <td>Actively selects viewpoints to maintain localization accuracy while moving.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2508.20981">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>123</td>
    <td>AnyPlace: Learning Generalizable Object Placement for Robot Manipulation</td>
    <td>Learns placement policies that generalize across objects and contexts.</td>
    <td><a href="https://any-place.github.io/">Link</a></td>
    <td><a href="https://arxiv.org/abs/2502.04531">link</a></td>
    <td><a href="https://github.com/ac-rad/anyplace/">Link</a></td>
  </tr>
  <tr>
    <td>124</td>
    <td>AutoEval: Autonomous Evaluation of Generalist Robot Manipulation Policies in the Real World</td>
    <td>Automates real-world evaluation of generalist manipulation policies with minimal setup.</td>
    <td><a href="https://auto-eval.github.io/">Link</a></td>
    <td><a href="https://arxiv.org/abs/2503.24278">link</a></td>
    <td><a href="https://github.com/zhouzypaul/auto_eval">Link</a></td>
  </tr>
  <tr>
    <td>125</td>
    <td>Poke and Strike: Learning Task-Informed Exploration Policies</td>
    <td>Designs exploration policies that “poke &amp; strike” based on task-informed signals.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2509.00178">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>126</td>
    <td>Ensuring Force Safety in Vision-Guided Robotic Manipulation via Implicit Tactile Calibration</td>
    <td>Improves force safety in vision-guided manipulation by implicitly calibrating tactile responses.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2412.10349">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>127</td>
    <td>Off Policy Lyapunov Stability in Reinforcement Learning</td>
    <td>Analyzes and enforces Lyapunov stability guarantees for off-policy RL controllers.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2509.09863">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>128</td>
    <td>ReCoDe: Reinforcement Learning-based Dynamic Constraint Design for Multi-Agent Coordination</td>
    <td>Learns task-dependent constraints to coordinate multiple agents more effectively.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2507.19151">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>129</td>
    <td>Human-like Navigation in a World Built for Humans</td>
    <td>Plans and navigates using priors that emulate human motion patterns and preferences.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/html/2509.21189v1">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>130</td>
    <td>Efficient Evaluation of Multi-Task Robot Policies With Active Experiment Selection</td>
    <td>Actively selects informative trials to evaluate multi-task policies with fewer experiments.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2502.09829">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>131</td>
    <td>Fail2Progress: Learning from Real-World Robot Failures with Stein Variational Inference</td>
    <td>Uses SVGD to turn failure experiences into policy improvements from real robot runs.</td>
    <td><a href="https://sites.google.com/view/fail2progress">Link</a></td>
    <td><a href="https://www.arxiv.org/abs/2509.01746">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>132</td>
    <td>ControlVLA: Few-shot Object-centric Adaptation for Pre-trained Vision-Language-Action Models</td>
    <td>Adapts pretrained VLA models to new objects/tasks with few-shot object-centric tuning.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2506.16211">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>133</td>
    <td>SafeBimanual: Diffusion-based trajectory optimization for safe bimanual manipulation</td>
    <td>Optimizes bimanual trajectories with diffusion priors while enforcing safety constraints.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2508.18268">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>134</td>
    <td>Junction State Estimation for Efficient Exploration in Reinforcement Learning</td>
    <td>Detects “junction” states to guide exploration and reduce sample complexity in RL.</td>
    <td>N/A</td>
    <td><a href="https://openreview.net/forum?id=NtnPVwUCAH#discussion">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>135</td>
    <td>QuaDreamer: Controllable Panoramic Video Generation for Quadruped Robots</td>
    <td>Generates controllable panoramic videos to train and evaluate quadruped perception stacks.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/html/2508.02512v1">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>136</td>
    <td>COMBO-Grasp: Learning Constraint-Based Manipulation for Bimanual Occluded Grasping</td>
    <td>Learned constraint-based policies for robust bimanual grasping under occlusion.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2502.08054">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>137</td>
    <td>D-Cubed: Latent Diffusion Trajectory Optimisation for Dexterous Deformable Manipulation</td>
    <td>Optimizes dexterous trajectories for deformables using latent diffusion models.</td>
    <td><a href="https://arxiv.org/abs/2403.12861">Link</a></td>
    <td><a href="https://arxiv.org/abs/2403.12861">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>138</td>
    <td>Long-VLA: Unleashing Long-Horizon Capability of Vision Language Action Model for Robot Manipulation</td>
    <td>Extends VLA planning capabilities to long-horizon manipulation tasks.</td>
    <td><a href="https://long-vla.github.io/">Link</a></td>
    <td><a href="https://arxiv.org/abs/2508.19958">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>139</td>
    <td>ImLPR: Image-based LiDAR Place Recognition using Vision Foundation Models</td>
    <td>Leverages VFM features for robust image-based LiDAR place recognition.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.18364">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>140</td>
    <td>MoTo: A Zero-shot Plug-in Interaction-aware Navigation for General Mobile Manipulation</td>
    <td>Zero-shot, plugin-based navigation that accounts for interactions in mobile manipulation.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2509.01658">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>141</td>
    <td>RobotxR1: Enabling Embodied Robotic Intelligence on Large Language Models through Closed-Loop Reinforcement Learning</td>
    <td>Combines LLM reasoning with closed-loop RL for embodied robotic tasks.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.03238">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>142</td>
    <td>GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data</td>
    <td>Pretrains a grasping foundation model on large-scale synthetic action sequences.</td>
    <td><a href="https://pku-epic.github.io/GraspVLA-web/">Link</a></td>
    <td><a href="https://arxiv.org/abs/2505.03233">link</a></td>
    <td><a href="https://github.com/PKU-EPIC/GraspVLA">Link</a></td>
  </tr>
  <tr>
    <td>143</td>
    <td>Learning from 10 Demos: Generalisable and Sample-Efficient Policy Learning with Oriented Affordance Frames</td>
    <td>Achieves generalization from ~10 demos using oriented affordance-frame supervision.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2410.12124">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>144</td>
    <td>Learning Long-Context Diffusion Policies via Past-Token Prediction</td>
    <td>Trains diffusion policies to leverage long past-context using next/past-token prediction.</td>
    <td><a href="https://long-context-dp.github.io/">Link</a></td>
    <td><a href="https://arxiv.org/abs/2505.09561">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>145</td>
    <td>Gen2Act: Human Video Generation in Novel Scenarios enables Generalizable Robot Manipulation</td>
    <td>Generates diverse human videos in novel scenes to bootstrap generalizable manipulation.</td>
    <td><a href="https://homangab.github.io/gen2act/">Link</a></td>
    <td><a href="">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>146</td>
    <td>Articulate AnyMesh: Open-vocabulary 3D Articulated Objects Modeling</td>
    <td>Models open-vocabulary articulated objects by inferring meshes and kinematic structure.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2502.02590">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>147</td>
    <td>Phantom: Training Robots Without Robots Using Only Human Videos</td>
    <td>Trains robot policies purely from human videos without robot data collection.</td>
    <td><a href="https://phantom-human-videos.github.io/">Link</a></td>
    <td><a href="https://arxiv.org/abs/2503.00779">link</a></td>
    <td><a href="https://github.com/MarionLepert/phantom">Link</a></td>
  </tr>
  <tr>
    <td>148</td>
    <td>Residual Neural Terminal Constraint for MPC-based Collision Avoidance in Dynamic Environments</td>
    <td>Adds a learned residual terminal constraint to MPC for safer collision avoidance.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2508.03428">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>149</td>
    <td>Hold My Beer: Learning Gentle Humanoid Locomotion and End-Effector Stabilization Control</td>
    <td>Trains humanoids for soft-contact locomotion with stabilized end-effector control.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.24198">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>150</td>
    <td>TReF-6: Inferring Task-Relevant Frames from a Single Demonstration for One-Shot Skill Generalization</td>
    <td>Infers task-relevant frames from a single demo to enable one-shot skill generalization.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2509.00310">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>151</td>
    <td>UniTac2Pose: A Unified Approach Learned in Simulation for Category-level Visuotactile In-hand Pose Estimation</td>
    <td>Unifies visual+tactile cues in sim to estimate in-hand pose at the category level.</td>
    <td>N/A</td>
    <td><a href="https://www.arxiv.org/abs/2509.15934">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>152</td>
    <td>Bipedal Balance Control with Whole-body Musculoskeletal Standing and Falling Simulations</td>
    <td>Uses musculoskeletal simulations of standing/falling to learn robust biped balance control.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2506.09383">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>153</td>
    <td>Do LLM Modules Generalize? A Study on Motion Generation for Autonomous Driving</td>
    <td>Evaluates whether LLM-based modules generalize to unseen driving motion generation tasks.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2509.02754">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>154</td>
    <td>AgentWorld: An Interactive Simulation Platform for Scene Construction and Mobile Robotic Manipulation</td>
    <td>Interactive platform for constructing scenes and training mobile manipulation policies.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2508.07770">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>155</td>
    <td>FastUMI: A Scalable and Hardware-Independent Universal Manipulation Interface with Dataset</td>
    <td>Hardware-agnostic manipulation interface with a large supporting dataset for scaling.</td>
    <td><a href="https://fastumi.com/">link</a></td>
    <td><a href="https://arxiv.org/abs/2409.19499">link</a></td>
    <td><a href="https://github.com/zxzm-zak/FastUMI_Data">link</a></td>
  </tr>
  <tr>
    <td>156</td>
    <td>Towards Generalizable Safety in Crowd Navigation via Conformal Uncertainty Handling</td>
    <td>Applies conformal uncertainty methods to ensure safety during crowd navigation.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2508.05634">link</a></td>
    <td><a href="https://github.com/tasl-lab/GenSafeNav">link</a></td>
  </tr>
  <tr>
    <td>157</td>
    <td>Reflective Planning: Vision-Language Models for Multi-Stage Long-Horizon Robotic Manipulation</td>
    <td>Uses VLMs with reflection to plan multi-stage long-horizon manipulation sequences.</td>
    <td><a href="https://reflect-vlm.github.io/">link</a></td>
    <td><a href="https://arxiv.org/abs/2502.16707">link</a></td>
    <td><a href="https://github.com/yunhaif/reflect-vlm">link</a></td>
  </tr>
  <tr>
    <td>158</td>
    <td>Unsupervised Skill Discovery as Exploration for Learning Agile Locomotion</td>
    <td>Discovers diverse skills for locomotion, turning exploration into agile behaviors.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2508.08982">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>159</td>
    <td>HyperTASR: Hypernetwork-Driven Task-Aware Scene Representations for Robust Manipulation</td>
    <td>Hypernetworks generate task-aware scene embeddings to improve manipulation robustness.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2508.18802">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>160</td>
    <td>GENNAV: Polygon Mask Generation for Generalized Referring Navigable Regions</td>
    <td>Generates polygon masks for referred navigable regions to guide embodied navigation.</td>
    <td><a href="https://gennav.vercel.app/">link</a></td>
    <td><a href="https://arxiv.org/abs/2508.21102">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>161</td>
    <td>PicoPose: Progressive Pixel-to-Pixel Correspondence Learning for Novel Object Pose Estimation</td>
    <td>Learns dense correspondences progressively to estimate 6D poses of novel objects.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2504.02617">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>162</td>
    <td>CHD: Coupled Hierarchical Diffusion for Long-Horizon Tasks</td>
    <td>Couples hierarchical policies with diffusion for scalable long-horizon task planning.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.07261">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>163</td>
    <td>BEVCalib: LiDAR-Camera Calibration via Geometry-Guided Bird’s-Eye View Representation</td>
    <td>Performs LiDAR–camera calibration using geometry cues in BEV space without markers.</td>
    <td><a href="https://cisl.ucr.edu/BEVCalib/">link</a></td>
    <td><a href="https://arxiv.org/abs/2506.02587">link</a></td>
    <td><a href="https://github.com/UCR-CISL/BEVCalib/tree/main">link</a></td>
  </tr>
  <tr>
    <td>164</td>
    <td>Latent Adaptive Planner for Dynamic Manipulation</td>
    <td>Plans dynamic manipulations by adapting in a learned latent space of strategies.</td>
    <td>N/A</td>
    <td><a href="https://www.arxiv.org/abs/2505.03077">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>165</td>
    <td>SLAC: Simulation-Pretrained Latent Action Space for Whole-Body Real-World RL</td>
    <td>Pretrains a latent action space in sim to accelerate whole-body RL on hardware.</td>
    <td><a href="https://robo-rl.github.io/">link</a></td>
    <td><a href="https://arxiv.org/abs/2506.04147">link</a></td>
    <td><a href="https://github.com/JiahengHu/SLAC">link</a></td>
  </tr>
  <tr>
    <td>166</td>
    <td>Neural Robot Dynamics</td>
    <td>Learns differentiable robot dynamics models that capture complex contact effects.</td>
    <td><a href="https://neural-robot-dynamics.github.io/">link</a></td>
    <td><a href="https://arxiv.org/abs/2508.15755">link</a></td>
    <td><a href="https://github.com/NVlabs/neural-robot-dynamics">link</a></td>
  </tr>
  <tr>
    <td>167</td>
    <td>GC-VLN: Instruction as Graph Constraints for Training-free Vision-and-Language Navigation</td>
    <td>Turns instructions into graph constraints to enable training-free VLN execution.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2509.10454">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>168</td>
    <td>Constrained Style Learning from Imperfect Demonstrations under Task Optimality</td>
    <td>Extracts task-consistent style from noisy demos while enforcing optimality constraints.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2507.09371">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>169</td>
    <td>TypeTele: Releasing Dexterity in Teleoperation by Dexterous Manipulation Types</td>
    <td>Teleop framework decomposed by manipulation “types” to increase dexterous capability.</td>
    <td><a href="https://isee-laboratory.github.io/TypeTele/">link</a></td>
    <td><a href="https://arxiv.org/abs/2507.01857">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>170</td>
    <td>VT-Refine: Learning Bimanual Assembly with Visuo-Tactile Feedback via Simulation Fine-Tuning</td>
    <td>Combines vision+tactile feedback with sim-finetuning for precise bimanual assembly.</td>
    <td>N/A</td>
    <td><a href="">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>171</td>
    <td>Adapt3R: Adaptive 3D Scene Representation for Domain Transfer in Imitation Learning</td>
    <td>Adapts 3D scene representations to new domains to improve imitation policy transfer.</td>
    <td>N/A</td>
    <td><a href="https://openreview.net/forum?id=mV3W5givYb">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>172</td>
    <td>FOMO-3D: Using Vision Foundation Models for Long-Tailed 3D Object Detection</td>
    <td>Leverages VFM priors to handle long-tailed distributions in 3D detection tasks.</td>
    <td>N/A</td>
    <td><a href="https://openreview.net/pdf/90ae722f9c63423d5379d06de77659df3e262f46.pdf">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>173</td>
    <td>Extracting Visual Plans from Unlabeled Videos via Symbolic Guidance</td>
    <td>Induces symbolic plans from unlabeled videos and converts them to robot-executable steps.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.08444">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>174</td>
    <td>Self-supervised perception for tactile skin covered dexterous hands</td>
    <td>Self-supervised learning for perception on dexterous hands covered with tactile skin.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.11420">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>175</td>
    <td>Predictive Red Teaming: Breaking Policies Without Breaking Robots</td>
    <td>“Red teams” robot policies in simulation to expose failures before real-world deployment.</td>
    <td><a href="https://predictive-red-team.github.io/">link</a></td>
    <td><a href="https://arxiv.org/abs/2502.06575">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>176</td>
    <td>Learning Long-Horizon Robot Manipulation Skills via Privileged Action</td>
    <td>Uses privileged action signals during training to acquire long-horizon manipulation skills.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2502.15442">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>177</td>
    <td>Towards Embodiment Scaling Laws in Robot Locomotion</td>
    <td>Empirical study of how embodiment and data scale affect locomotion policy performance.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.05753">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>178</td>
    <td>O$^3$Afford: One-Shot 3D Object-to-Object Affordance Grounding for Generalizable Robotic Manipulation</td>
    <td>One-shot affordance grounding between novel 3D objects to generalize manipulation skills.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/html/2509.06233v1">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>179</td>
    <td>Estimating Value of Assistance for Online POMDP Robotic Agents</td>
    <td>Quantifies assistance value online to decide when human help benefits a POMDP agent.</td>
    <td>N/A</td>
    <td><a href="https://dyalab.mines.edu/2025/icra-workshop/6.pdf">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>180</td>
    <td>Shortcut Learning in Generalist Robot Policies: The Role of Dataset Diversity and Fragmentation</td>
    <td>Analyzes shortcut biases and how dataset diversity/fragmentation impact generalist policies.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2508.06426">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>181</td>
    <td>SDS – See it, Do it, Sorted: Quadruped Skill Synthesis from Single Video Demonstration</td>
    <td>Derives quadruped skills from a single video demo using perception-to-action synthesis.</td>
    <td><a href="https://rpl-cs-ucl.github.io/SDSweb/">link</a></td>
    <td><a href="https://arxiv.org/abs/2410.11571">link</a></td>
    <td><a href="https://github.com/RPL-CS-UCL/SDS">link</a></td>
  </tr>
  <tr>
    <td>182</td>
    <td>TrackVLA: Embodied Visual Tracking in the Wild</td>
    <td>Embeds tracking into VLA frameworks for robust in-the-wild target following.</td>
    <td><a href="https://pku-epic.github.io/TrackVLA-web/">link</a></td>
    <td><a href="https://arxiv.org/abs/2505.23189">link</a></td>
    <td><a href="https://github.com/wsakobe/TrackVLA">link</a></td>
  </tr>
  <tr>
    <td>183</td>
    <td>GLOVER++: Unleashing the Potential of Affordance Learning from Human Behaviors for Robotic Manipulation</td>
    <td>Leverages human behavior data to improve affordance learning for manipulation tasks.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.11865">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>184</td>
    <td>LaDi-WM: A Latent Diffusion-Based World Model for Predictive Manipulation</td>
    <td>Uses latent diffusion world models to predict future states for manipulation planning.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.11528">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>185</td>
    <td>ManiFlow: A General Robot Manipulation Policy via Consistency Flow Training</td>
    <td>Trains a general manipulation policy using consistency-based flow training objectives.</td>
    <td><a href="https://maniflow-policy.github.io/">link</a></td>
    <td><a href="https://www.arxiv.org/abs/2509.01819">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>186</td>
    <td>Robot Learning from Any Images</td>
    <td>Bootstraps visuomotor skills from broad internet-scale images via clever supervision.</td>
    <td>N/A</td>
    <td><a href="https://www.arxiv.org/abs/2509.22970">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>187</td>
    <td>Deep Reactive Policy: Learning Reactive Manipulator Motion Planning for Dynamic Environments</td>
    <td>End-to-end learned reactive planner for manipulators operating in dynamic scenes.</td>
    <td><a href="https://deep-reactive-policy.com/">link</a></td>
    <td><a href="https://arxiv.org/abs/2509.06953">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>188</td>
    <td>CASPER: Inferring Diverse Intents for Assistive Teleoperation with Vision Language Models</td>
    <td>Infers user intent distributions via VLMs to assist teleoperation effectively.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2506.14727">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>189</td>
    <td>UniSkill: Imitating Human Videos via Cross-Embodiment Skill Representations</td>
    <td>Transfers skills from human videos to robots using cross-embodiment representations.</td>
    <td><a href="https://kimhanjung.github.io/UniSkill/">link</a></td>
    <td><a href="https://arxiv.org/abs/2505.08787">link</a></td>
    <td><a href="https://github.com/KimHanjung/UniSkill">link</a></td>
  </tr>
  <tr>
    <td>190</td>
    <td>ParticleFormer: A 3D Point Cloud World Model for Multi-Object, Multi-Material Robotic Manipulation</td>
    <td>Particle-based world model on point clouds to handle multi-object, multi-material dynamics.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2506.23126">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>191</td>
    <td>Crossing the Human-Robot Embodiment Gap with Sim-to-Real RL using One Human Demonstration</td>
    <td>Bridges human-to-robot embodiment gap via sim-to-real RL seeded by one human demo.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2504.12609">link</a></td>
    <td><a href="https://github.com/tylerlum/human2sim2robot">link</a></td>
  </tr>
  <tr>
    <td>192</td>
    <td>One Demo is Worth a Thousand Trajectories: Action-View Augmentation for Visuomotor Policies</td>
    <td>Action-view augmentation from a single demo to train robust visuomotor policies.</td>
    <td>N/A</td>
    <td><a href="https://openreview.net/pdf?id=Hu3NoPMAg4">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>193</td>
    <td>VLM-AD: End-to-End Autonomous Driving through Vision-Language Model Supervision</td>
    <td>Supervises end-to-end driving policies using high-level VLM reasoning signals.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2412.14446">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>194</td>
    <td>LLM-Guided Probabilistic Program Induction for POMDP Model Estimation</td>
    <td>Uses LLMs to induce probabilistic programs that estimate POMDP dynamics/observations.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2505.02216">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>195</td>
    <td>Vision in Action: Learning Active Perception from Human Demonstrations</td>
    <td>Learns active perception strategies from human demos for better task performance.</td>
    <td><a href="https://vision-in-action.github.io/">link</a></td>
    <td><a href="https://arxiv.org/abs/2506.15666">link</a></td>
    <td><a href="https://github.com/haoyu-x/vision-in-action">link</a></td>
  </tr>
  <tr>
    <td>196</td>
    <td>Point Policy: Unifying Observations and Actions with Key Points for Robot Manipulation</td>
    <td>Represents both observations and actions as keypoints to simplify manipulation learning.</td>
    <td><a href="https://point-policy.github.io/">link</a></td>
    <td><a href="https://arxiv.org/abs/2502.20391">link</a></td>
    <td><a href="https://github.com/siddhanthaldar/Point-Policy">link</a></td>
  </tr>
  <tr>
    <td>197</td>
    <td>From Tabula Rasa to Emergent Abilities: Discovering Robot Skills via Real-World Unsupervised Quality-Diversity</td>
    <td>Discovers diverse real-world skills via unsupervised quality-diversity search.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2508.19172">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>198</td>
    <td>Robust Dexterous Grasping of General Objects</td>
    <td>Designs policies for robust dexterous grasping across varied, previously unseen objects.</td>
    <td><a href="https://zdchan.github.io/Robust_DexGrasp/">link</a></td>
    <td><a href="https://arxiv.org/abs/2504.05287">link</a></td>
    <td><a href="https://github.com/zdchan/RobustDexGrasp">link</a></td>
  </tr>
  <tr>
    <td>199</td>
    <td>Sim-to-Real Reinforcement Learning for Vision-Based Dexterous Manipulation on Humanoids</td>
    <td>Transfers vision-based dexterous manipulation skills onto humanoid platforms via RL.</td>
    <td><a href="https://toruowo.github.io/recipe/">link</a></td>
    <td><a href="https://arxiv.org/abs/2502.20396">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>200</td>
    <td>Humanoid Policy ~ Human Policy</td>
    <td>Explores alignment between humanoid robot policies and human-like control strategies.</td>
    <td><a href="https://human-as-robot.github.io/">link</a></td>
    <td><a href="https://arxiv.org/abs/2503.13441">link</a></td>
    <td><a href="https://github.com/RogerQi/human-policy">link</a></td>
  </tr>
    <tr>
    <td>201</td>
    <td>ToddlerBot: Open-Source ML-Compatible Humanoid Platform for Loco-Manipulation</td>
    <td>Open-source humanoid platform designed for learning-based loco-manipulation research.</td>
    <td><a href="https://toddlerbot.github.io/">link</a></td>
    <td><a href="https://arxiv.org/abs/2502.00893">link</a></td>
    <td><a href="https://github.com/hshi74/toddlerbot">link</a></td>
  </tr>
  <tr>
    <td>202</td>
    <td>TWIST: Teleoperated Whole-Body Imitation System</td>
    <td>Teleoperation framework enabling full-body imitation to collect rich humanoid data.</td>
    <td><a href="https://yanjieze.com/TWIST/">link</a></td>
    <td><a href="https://arxiv.org/abs/2505.02833">link</a></td>
    <td><a href="https://github.com/YanjieZe/TWIST">link</a></td>
  </tr>
  <tr>
    <td>203</td>
    <td>exUMI: Extensible Robot Teaching System with Action-aware Task-agnostic Tactile Representation</td>
    <td>Extensible teaching system that learns task-agnostic tactile reps aware of action context.</td>
    <td>N/A</td>
    <td><a href="https://www.arxiv.org/abs/2509.14688">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>204</td>
    <td>OPAL: Visibility-aware LiDAR-to-OpenStreetMap Place Recognition via Adaptive Radial Fusion</td>
    <td>Performs LiDAR↔OSM place recognition using visibility-aware, radial fusion in BEV.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2504.19258">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>205</td>
    <td>CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal Diffusion</td>
    <td>Introduces causal diffusion to stabilize autoregressive visuomotor policy learning.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2506.14769">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>206</td>
    <td>Robot Trains Robot: Automatic Real-World Policy Adaptation and Learning for Humanoids</td>
    <td>Humanoids auto-adapt policies in the real world via self-training and evaluation loops.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2508.12252">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>207</td>
    <td>See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation</td>
    <td>Learning-free VLM pipeline that parses language/vision prompts for UAV navigation targets.</td>
     <td><a href="https://spf-web.pages.dev/">link</a></td>
    <td><a href="https://www.arxiv.org/abs/2509.22653">link</a></td>
    <td><a href="https://github.com/Hu-chih-yao/see-point-fly">link</a></td>
  </tr>
  <tr>
    <td>208</td>
    <td>LaVA-Man: Learning Visual Action Representations for Robot Manipulation</td>
    <td>Learns visual action embeddings to improve generalization in robot manipulation.</td>
    <td>N/A</td>
    <td><a href="https://www.arxiv.org/abs/2508.19391">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>209</td>
    <td>3DS-VLA: A 3D Spatial-Aware Vision Language Action Model for Robust Multi-Task Manipulation</td>
    <td>VLA model with explicit 3D spatial awareness for robust, multi-task manipulation.</td>
    <td>N/A</td>
    <td><a href="https://openreview.net/pdf?id=dT45OMevL5">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>210</td>
    <td>Uncertainty-aware Accurate Elevation Modeling for Off-road Navigation via Neural Processes</td>
    <td>Neural processes produce uncertainty-aware elevation maps for safer off-road planning.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2508.03890">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>211</td>
    <td>Generalist Robot Manipulation beyond Action Labeled Data</td>
    <td>Trains generalist manipulation policies without relying on explicit action labels.</td>
     <td><a href="https://motovla.github.io/">link</a></td>
    <td><a href="https://arxiv.org/abs/2509.19958">link</a></td>
    <td><a href="https://github.com/insait-institute/motovla">link</a></td>
  </tr>
  <tr>
    <td>212</td>
    <td>BranchOut: Capturing Realistic Multimodality in Autonomous Driving Decisions</td>
    <td>Models branching, multimodal futures to better capture diverse driving decisions.</td>
    <td>N/A</td>
    <td><a href="https://openreview.net/pdf/8a87e7f37431a21394eac9a184d3461226abe4c7.pdf">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>213</td>
    <td>Co-Design of Soft Gripper with Neural Physics</td>
    <td>Jointly optimizes soft gripper design and control with differentiable neural physics.</td>
     <td><a href="https://yswhynot.github.io/codesign-soft/">link</a></td>
    <td><a href="https://arxiv.org/abs/2505.20404">link</a></td>
   <td><a href="https://github.com/yswhynot/codesign-soft-gripper">link</a></td>
  </tr>
  <tr>
    <td>214</td>
    <td>Elucidating the Design Space of Torque-aware Vision-Language-Action Models</td>
    <td>Systematic study of torque-aware VLA design choices for manipulation performance.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2509.07962">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>215</td>
    <td>RoboChemist: Long-Horizon and Safety-Compliant Robotic Chemical Experimentation</td>
    <td>Automates long-horizon chemical experiments with explicit safety constraints.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2509.08820">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>216</td>
    <td>COLLAGE: Adaptive Fusion-based Retrieval for Augmented Policy Learning</td>
    <td>Retrieves and fuses relevant experience adaptively to augment policy learning.</td>
    <td><a href="https://robin-lab.cs.utexas.edu/COLLAGE/">link</a></td>
    <td><a href="https://arxiv.org/abs/2508.01131">link</a></td>
    <td><a href="https://github.com/SateeshKumar21/collage-retrieval-code">link</a></td>
  </tr>
  <tr>
    <td>217</td>
    <td>GraspMolmo: Generalizable Task-Oriented Grasping via Large-Scale Synthetic Data Generation</td>
    <td>Generates large-scale synthetic data to train task-oriented grasping policies.</td>
     <td><a href="https://abhaybd.github.io/GraspMolmo/">link</a></td>
    <td><a href="https://arxiv.org/abs/2505.13441">link</a></td>
     <td><a href="https://github.com/abhaybd/GraspMolmo">link</a></td>
  </tr>
  <tr>
    <td>218</td>
    <td>Motion Blender Gaussian Splatting for Dynamic Reconstruction</td>
    <td>Combines motion blending with Gaussian splats to reconstruct dynamic scenes.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2503.09040">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>219</td>
    <td>Improving Efficiency of Sampling-based Motion Planning via Message-Passing Monte Carlo</td>
    <td>Message-passing Monte Carlo reduces variance and improves sampling-based planning efficiency.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2410.03909">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>220</td>
    <td>CaRL: Learning Scalable Planning Policies with Simple Rewards</td>
    <td>Scales planning policies using simple reward structures with strong generalization.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2504.17838">link</a></td>
    <td>N/A</td>
  </tr>
  <tr>
    <td>221</td>
    <td>Pseudo-Simulation for Autonomous Driving</td>
    <td>Uses pseudo-simulated data to train and evaluate driving policies more efficiently.</td>
    <td>N/A</td>
    <td><a href="https://arxiv.org/abs/2506.04218">link</a></td>
    <td><a href="https://github.com/autonomousvision/navsim">link</a></td>
  </tr>
</table>
